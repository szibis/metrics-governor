{{- if .Values.alerting.enabled }}
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: {{ include "metrics-governor.fullname" . }}
  {{- if .Values.alerting.namespace }}
  namespace: {{ .Values.alerting.namespace }}
  {{- else }}
  namespace: {{ .Release.Namespace }}
  {{- end }}
  labels:
    {{- include "metrics-governor.labels" . | nindent 4 }}
    {{- with .Values.alerting.additionalLabels }}
    {{- toYaml . | nindent 4 }}
    {{- end }}
  {{- with .Values.alerting.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  groups:
    - name: metrics-governor
      {{- if .Values.alerting.interval }}
      interval: {{ .Values.alerting.interval }}
      {{- end }}
      rules:

        {{- if not (has "MetricsGovernorDown" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorDown
          expr: up{job=~".*metrics-governor.*"} == 0
          for: 2m
          labels:
            severity: critical
          annotations:
            summary: "metrics-governor is down"
            description: "{{`{{ $labels.instance }}`}} has been unreachable for 2 minutes."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernordown"
        {{- end }}

        {{- if not (has "MetricsGovernorDataLoss" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorDataLoss
          expr: rate(metrics_governor_export_data_loss_total[5m]) > 0
          for: 1m
          labels:
            severity: critical
          annotations:
            summary: "metrics-governor is losing data"
            description: "{{`{{ $labels.instance }}`}} is dropping batches permanently."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernordataloss"
        {{- end }}

        {{- if not (has "MetricsGovernorExportDegraded" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorExportDegraded
          expr: |
            (
              rate(metrics_governor_otlp_export_errors_total[5m])
              / clamp_min(rate(metrics_governor_otlp_export_requests_total[5m]), 0.001)
            ) > {{ .Values.alerting.thresholds.exportErrorRate | default 0.1 }}
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor export error rate elevated"
            description: "{{`{{ $labels.instance }}`}} has elevated export failure rate."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorexportdegraded"
        {{- end }}

        {{- if not (has "MetricsGovernorQueueSaturated" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorQueueSaturated
          expr: |
            (
              metrics_governor_queue_bytes
              / clamp_min(metrics_governor_queue_max_bytes, 1)
            ) > {{ .Values.alerting.thresholds.queueUtilization | default 0.85 }}
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor queue filling up"
            description: "{{`{{ $labels.instance }}`}} queue at {{`{{ $value | humanizePercentage }}`}}."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorqueuesaturated"
        {{- end }}

        {{- if not (has "MetricsGovernorCircuitOpen" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorCircuitOpen
          expr: metrics_governor_circuit_breaker_state{state="open"} == 1
          for: 2m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor circuit breaker is open"
            description: "{{`{{ $labels.instance }}`}} circuit breaker open for 2 minutes."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorcircuitopen"
        {{- end }}

        {{- if not (has "MetricsGovernorOOMRisk" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorOOMRisk
          expr: |
            (
              go_memstats_heap_alloc_bytes{job=~".*metrics-governor.*"}
              / clamp_min(metrics_governor_memory_limit_bytes, 1)
            ) > {{ .Values.alerting.thresholds.memoryUsage | default 0.90 }}
          for: 5m
          labels:
            severity: critical
          annotations:
            summary: "metrics-governor memory approaching limit"
            description: "{{`{{ $labels.instance }}`}} heap at {{`{{ $value | humanizePercentage }}`}}."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernoroomrisk"
        {{- end }}

        {{- if not (has "MetricsGovernorBackpressure" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorBackpressure
          expr: |
            (
              rate(metrics_governor_buffer_rejected_total[5m])
              + rate(metrics_governor_buffer_evictions_total[5m])
            ) > {{ .Values.alerting.thresholds.backpressureRate | default 1 }}
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor buffer under pressure"
            description: "{{`{{ $labels.instance }}`}} buffer rejecting or evicting data."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorbackpressure"
        {{- end }}

        {{- if not (has "MetricsGovernorWorkersSaturated" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorWorkersSaturated
          expr: |
            (
              metrics_governor_queue_workers_active
              / clamp_min(metrics_governor_queue_workers_total, 1)
            ) > {{ .Values.alerting.thresholds.workerUtilization | default 0.90 }}
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor workers saturated"
            description: "{{`{{ $labels.instance }}`}} at {{`{{ $value | humanizePercentage }}`}} worker utilization."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorworkerssaturated"
        {{- end }}

        {{- if not (has "MetricsGovernorCardinalityExplosion" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorCardinalityExplosion
          expr: rate(metrics_governor_limit_cardinality_exceeded_total[10m]) > 0
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor cardinality limits exceeded"
            description: "{{`{{ $labels.instance }}`}} sustained cardinality violations."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorcardinalityexplosion"
        {{- end }}

        {{- if not (has "MetricsGovernorConfigStale" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorConfigStale
          expr: (time() - metrics_governor_limits_config_reload_last_success_timestamp_seconds) > {{ .Values.alerting.thresholds.configStaleness | default 86400 }}
          for: 1h
          labels:
            severity: info
          annotations:
            summary: "metrics-governor config not reloaded recently"
            description: "{{`{{ $labels.instance }}`}} config may be stale."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorconfigstale"
        {{- end }}

        {{- if not (has "MetricsGovernorProcessingOverloaded" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorProcessingOverloaded
          expr: |
            histogram_quantile(0.99, rate(metrics_governor_processing_duration_seconds_bucket[5m]))
            > {{ .Values.alerting.thresholds.processingDurationP99 | default 0.1 }}
          for: 5m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor processing rules taking too long"
            description: "{{`{{ $labels.instance }}`}} P99 processing duration > {{`{{ $value | humanizeDuration }}`}}."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorprocessingoverloaded"
        {{- end }}

        {{- if not (has "MetricsGovernorAggregateGroupExplosion" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorAggregateGroupExplosion
          expr: sum(metrics_governor_processing_aggregate_groups_active) > {{ .Values.alerting.thresholds.aggregateGroupLimit | default 100000 }}
          for: 10m
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor too many aggregate groups"
            description: "{{`{{ $labels.instance }}`}} has {{`{{ $value }}`}} active aggregate groups."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernoraggregategroupexplosion"
        {{- end }}

        {{- if not (has "MetricsGovernorProcessingHighDropRate" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorProcessingHighDropRate
          expr: |
            rate(metrics_governor_processing_dropped_datapoints_total[5m])
            / clamp_min(rate(metrics_governor_processing_input_datapoints_total[5m]), 0.001)
            > {{ .Values.alerting.thresholds.processingDropRate | default 0.95 }}
          for: 5m
          labels:
            severity: info
          annotations:
            summary: "metrics-governor processing dropping >95% of datapoints"
            description: "{{`{{ $labels.instance }}`}} processing rules are very aggressive â€” verify this is intentional."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/alerts/README.md#metricsgovernorprocessinghighdroprate"
        {{- end }}

        {{- if not (has "MetricsGovernorSpilloverCascade" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorSpilloverCascade
          expr: metrics_governor_spillover_active == 1
          for: {{ .Values.alerting.thresholds.spilloverCascadeFor | default "2m" }}
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor spillover cascade active"
            description: "{{`{{ $labels.instance }}`}} hybrid queue in spillover mode for 2+ minutes."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/docs/alerting.md#metricsgovernorspillovercascade"
        {{- end }}

        {{- if not (has "MetricsGovernorLoadSheddingActive" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorLoadSheddingActive
          expr: rate(metrics_governor_receiver_load_shedding_total[2m]) > 0
          for: {{ .Values.alerting.thresholds.loadSheddingFor | default "2m" }}
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor is shedding load"
            description: "{{`{{ $labels.instance }}`}} rejecting requests due to pipeline pressure."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/docs/alerting.md#metricsgovernorloadsheddingactive"
        {{- end }}

        {{- if not (has "MetricsGovernorStatsDegraded" .Values.alerting.disabledAlerts) }}
        - alert: MetricsGovernorStatsDegraded
          expr: metrics_governor_stats_level_current < metrics_governor_stats_level_configured
          for: {{ .Values.alerting.thresholds.statsDegradedFor | default "5m" }}
          labels:
            severity: warning
          annotations:
            summary: "metrics-governor stats level degraded"
            description: "{{`{{ $labels.instance }}`}} stats auto-downgraded due to memory pressure."
            runbook_url: "https://github.com/your-org/metrics-governor/blob/main/docs/alerting.md#metricsgovernorstatsdegraded"
        {{- end }}

        {{- with .Values.alerting.additionalRules }}
        {{- toYaml . | nindent 8 }}
        {{- end }}
{{- end }}
