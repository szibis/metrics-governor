# Autotune — Safe Threshold Discovery from Real Traffic

## Context

Users deploying metrics-governor today must manually configure limits rules (`max_cardinality`, `max_datapoints_rate`) and processing rules (sampling, downsampling, aggregation, drop). Getting these wrong means either blocking legitimate traffic or providing no protection. The autotune feature solves this by observing real traffic in a learning mode (dry-run by default), computing safe thresholds from actual patterns, and optionally auto-applying them — all while logging every action and exposing full observability.

### User Requirements (from conversation)

- Covers both limits and processing rule suggestions
- Auto-apply by updating config + structured logs + Prometheus metrics + `/autotune` JSON endpoint
- Write suggested YAML config files
- Support both time-bounded learning windows AND continuous convergence
- Pluggable external providers for real-time backend intelligence (first: VictoriaMetrics)
- Grafana Adaptive Metrics-style insights — unused metrics detection, top growing cardinality, label optimization, last usage tracking

---

## Design Philosophy: Self-Hosted Adaptive Metrics

This feature is inspired by [Grafana Adaptive Metrics](https://grafana.com/docs/grafana-cloud/adaptive-telemetry/adaptive-metrics/) but designed as an open-source, self-hosted alternative that works with any backend. Key differences:

|                 | Grafana Adaptive Metrics              | metrics-governor Autotune                                                 |
|-----------------|---------------------------------------|---------------------------------------------------------------------------|
| Deployment      | SaaS only (Grafana Cloud)             | Self-hosted, any environment                                              |
| Data source     | Internal Grafana query logs           | Internal proxy stats + pluggable external APIs                            |
| Usage detection | Grafana dashboards/alerts/rules       | Backend API (VM metric_names_stats) + proxy query pass-through            |
| Actions         | Aggregation rules (label dropping)    | Full spectrum: limits, sampling, downsampling, aggregation, drop, relabel |
| Scope           | Prometheus metrics only               | OTLP + PRW dual-protocol                                                 |
| Cost            | Per-metric pricing                    | Free (part of governor)                                                   |

### Intelligence Signals (going beyond basic P99 thresholds)

The Analyzer consumes 5 classes of signals to generate smart recommendations:

1. **Internal proxy stats** — cardinality, rate, label counts per metric (from `stats.Collector`)
2. **Limits enforcement data** — violations, near-violations, dry-run hits (from `limits.Enforcer`)
3. **Backend cardinality** — total active series, top-N by series count (from VM `/api/v1/status/tsdb`)
4. **Metric usage/query frequency** — requests count + last request time per metric (from VM `/api/v1/status/metric_names_stats`)
5. **Cardinality growth rate** — delta between snapshots over time (computed internally)

---

## Design Constraint: Manual Rules Are Sacred

Autotune must never override, modify, or conflict with manually-defined rules. Manual rules represent deliberate operator decisions and take absolute precedence.

### How it works

1. **Discovery**: On startup and each reload, autotune reads the current limits config (`limits.LimitsConfig.Rules`) and processing config (`sampling.ProcessingConfig.Rules`)
2. **Fingerprinting**: Each rule gets a source tag — `"manual"` (loaded from config file) or `"autotune"` (generated by this system)
3. **Exclusion**: The Analyzer skips any metric that already matches a manual rule's match pattern. It only generates suggestions for uncovered metrics.
4. **Non-destructive apply**: When applying, autotune appends new rules (prefixed `autotune_`) alongside manual rules. It never modifies or deletes manual rules.
5. **Conflict detection**: If a manual rule is added after autotune already covers that metric, the autotune rule is automatically retired on next cycle and logged.

### Exemptions config

Users can also explicitly exempt metrics from autotune via config:

```yaml
autotune:
  exempt_metrics:
    - "kube_.*"              # never touch Kubernetes metrics
    - "prometheus_.*"        # never touch Prometheus internal metrics
  exempt_labels:
    - "instance"             # never suggest stripping instance label
```

This mirrors [Grafana's recommendation management](https://grafana.com/docs/grafana-cloud/adaptive-telemetry/adaptive-metrics/manage-recommendations/) where users protect critical metrics from automated changes.

---

## Caching Architecture

External API calls are expensive and rate-limited. All provider data and computed analysis results are cached to:
- Avoid hammering external endpoints
- Provide data during temporary provider outages
- Enable fast `/autotune` endpoint responses
- Support offline analysis after learning window

### Cache Layers

```
┌──────────────────────────────────────────────────┐
│  L1: In-Memory Provider Cache (per provider)     │
│  TTL: provider.interval (default 60s)            │
│  Content: raw ExternalData from last Fetch()     │
│  On miss: call provider.Fetch()                  │
│  On error: serve stale data + log warning        │
├──────────────────────────────────────────────────┤
│  L2: Observation Ring Buffer (Observer)          │
│  Size: max_snapshots (default 240 = 2h @ 30s)   │
│  Content: merged Snapshot (internal + external)  │
│  Eviction: oldest-first ring buffer              │
├──────────────────────────────────────────────────┤
│  L3: Analysis Results Cache (Engine)             │
│  TTL: converge_interval (default 15m)            │
│  Content: last Suggestions + insights            │
│  Served by: /autotune HTTP endpoint (instant)    │
├──────────────────────────────────────────────────┤
│  L4: Persistent Snapshot File (optional)         │
│  Path: autotune_state_path (alongside output)    │
│  Content: JSON dump of snapshots + suggestions   │
│  Purpose: survive restarts, resume learning      │
└──────────────────────────────────────────────────┘
```

### Provider Cache Implementation

```go
type cachedProvider struct {
    inner    Provider
    mu       sync.RWMutex
    cached   *ExternalData
    cachedAt time.Time
    ttl      time.Duration
    staleOK  bool  // serve stale data on error (default: true)
}

func (c *cachedProvider) Fetch(ctx context.Context) (*ExternalData, error) {
    c.mu.RLock()
    if c.cached != nil && time.Since(c.cachedAt) < c.ttl {
        defer c.mu.RUnlock()
        return c.cached, nil  // L1 cache hit
    }
    c.mu.RUnlock()

    data, err := c.inner.Fetch(ctx)
    if err != nil {
        c.mu.RLock()
        if c.staleOK && c.cached != nil {
            defer c.mu.RUnlock()
            return c.cached, nil  // serve stale on error
        }
        c.mu.RUnlock()
        return nil, err
    }

    c.mu.Lock()
    c.cached = data
    c.cachedAt = time.Now()
    c.mu.Unlock()
    return data, nil
}
```

Every `Provider` is automatically wrapped in `cachedProvider` by the factory — no opt-in needed.

### State Persistence (L4)

Optional file-based persistence for learning state:

```yaml
autotune:
  state_path: /var/lib/metrics-governor/autotune-state.json
  state_compression: zstd  # compress persistent state (zstd|gzip|none, default: zstd)
```

- On shutdown: dump current snapshots + suggestions to zstd-compressed JSON (saves ~80% disk). With zstd level 6, a 2 MB state file compresses to ~400 KB on disk.
- On startup: detect compression from file header, decompress, and reload state.
- On startup: reload state, skip re-learning if enough samples exist. This prevents losing a 1-hour learning window on pod restart.

---

## Network Optimization: Compression & Streaming

With 5-10+ governor pods exchanging autotune data, plus regular VM API polling, network efficiency is critical. We reuse the existing `internal/compression/` package (gzip, zstd, snappy via `klauspost/compress` with `sync.Pool`-based encoder pooling) and add streaming JSON parsing for large responses.

### Compression Strategy by Channel

| Channel                        | Compression                | Rationale                                                                                  |
|--------------------------------|----------------------------|--------------------------------------------------------------------------------------------|
| Peer `/autotune/local`         | zstd (level 3)             | Best ratio for JSON at minimal CPU; peer-to-peer is same-cluster, latency matters less     |
| VM `/api/v1/status/tsdb`       | gzip (Accept-Encoding)     | VM natively supports gzip on API responses; ~4 MB → ~400 KB                                |
| VM `/api/v1/status/metric_names_stats` | gzip (Accept-Encoding) | Same — ~2.5 MB → ~250 KB                                                                  |
| `/autotune` global endpoint    | gzip (Accept-Encoding negotiation) | Served to browsers/curl; gzip universally supported                                |
| `/autotune/ui` HTML            | gzip (Accept-Encoding negotiation) | Static HTML, ~400 LOC compresses to ~5 KB                                          |
| State persistence file         | zstd (level 6)             | Best offline ratio; disk I/O not latency-sensitive                                         |

### Peer-to-Peer Compressed Exchange

Reuses the existing `internal/compression/` pooled writers/readers:

```go
// Server side: compress /autotune/local responses
func (h *autotuneHandler) serveLocal(w http.ResponseWriter, r *http.Request) {
    data := h.engine.LocalState()
    jsonBytes, _ := json.Marshal(data)

    enc := negotiateEncoding(r.Header.Get("Accept-Encoding")) // zstd > gzip > identity
    if enc != compression.TypeNone {
        w.Header().Set("Content-Encoding", enc.ContentEncoding())
        compressed, _ := compression.Compress(jsonBytes, compression.Config{Type: enc})
        w.Write(compressed)
        return
    }
    w.Write(jsonBytes)
}

// Client side: request + decompress peer responses
func (g *GlobalAggregator) fetchPeer(ctx context.Context, peer string) (*CompactPeerState, error) {
    req, _ := http.NewRequestWithContext(ctx, "GET", peer+"/autotune/local", nil)
    req.Header.Set("Accept-Encoding", "zstd, gzip") // prefer zstd

    resp, err := g.client.Do(req)
    // ...
    body, _ := io.ReadAll(resp.Body)
    if ce := resp.Header.Get("Content-Encoding"); ce != "" {
        enc, _ := compression.ParseContentEncoding(ce)
        body, _ = compression.Decompress(body, enc)
    }
    var state CompactPeerState
    json.Unmarshal(body, &state)
    return &state, nil
}
```

The `compression.Compress` / `compression.Decompress` functions already use pooled encoders, so this adds zero allocation overhead per call.

### VM API Response Compression

VictoriaMetrics supports `Accept-Encoding: gzip` on all API endpoints. The VM provider client requests compressed responses:

```go
func (v *vmProvider) fetch(ctx context.Context, path string) ([]byte, error) {
    req, _ := http.NewRequestWithContext(ctx, "GET", v.endpoint+path, nil)
    req.Header.Set("Accept-Encoding", "gzip")  // VM supports gzip

    resp, err := v.client.Do(req)
    // ...
    body, _ := io.ReadAll(resp.Body)
    if resp.Header.Get("Content-Encoding") == "gzip" {
        body, _ = compression.Decompress(body, compression.TypeGzip)
    }
    return body, nil
}
```

**Impact**: VM tsdb response 4 MB → ~400 KB (gzip ~10:1 on JSON). VM metric_names_stats 2.5 MB → ~250 KB. Total VM bandwidth drops from 109 KB/s to ~11 KB/s.

### Streaming JSON Parsing for VM Responses

For large VM responses (50K+ metrics), full-buffer JSON parsing uses ~10 MB of peak memory. Instead, use streaming `json.Decoder` to parse the top-N entries incrementally:

```go
func (v *vmProvider) parseStreamingTSDB(r io.Reader) (*ExternalData, error) {
    dec := json.NewDecoder(r)
    data := &ExternalData{MetricStats: make(map[string]ExternalMetricStat, v.topN)}

    // Navigate into: {"status":"success","data":{"seriesCountByMetricName":[...]}}
    for dec.More() {
        tok, _ := dec.Token()
        if tok == "seriesCountByMetricName" {
            dec.Token() // consume '['
            for dec.More() {
                var entry struct{ Name string; Value int64 }
                dec.Decode(&entry)
                data.MetricStats[entry.Name] = ExternalMetricStat{
                    SeriesCount: entry.Value,
                }
            }
            dec.Token() // consume ']'
        }
        // ... parse other fields similarly
    }
    return data, nil
}
```

**Impact**: Peak memory for VM response parsing drops from ~10 MB (full buffer) to ~2 MB (streaming + final map). Parsing latency stays similar since JSON is read sequentially either way.

### Connection Pooling for Peers + VM

The HTTP client for both peer sync and VM polling reuses the existing project patterns from `internal/exporter/prw_exporter.go`:

```go
func newAutotuneTransport(maxPeers int) *http.Transport {
    return &http.Transport{
        Proxy: http.ProxyFromEnvironment,
        DialContext: (&net.Dialer{
            Timeout:   5 * time.Second,
            KeepAlive: 30 * time.Second,
        }).DialContext,
        MaxIdleConns:          maxPeers + 5,  // peers + VM + headroom
        MaxIdleConnsPerHost:   2,             // at most 2 idle per peer
        IdleConnTimeout:       90 * time.Second,
        DisableCompression:    true,          // we handle compression manually
        WriteBufferSize:       4 * 1024,      // small — we mostly read
        ReadBufferSize:        64 * 1024,     // large — responses are big
        TLSHandshakeTimeout:  5 * time.Second,
        ResponseHeaderTimeout: 5 * time.Second,
    }
}
```

**Key**: `DisableCompression: true` prevents Go's automatic decompression (matching the project's convention in `prw_exporter.go:180`), letting us use the pooled `compression.Decompress()` instead.

### Updated Network Impact (with compression)

| Channel                           | Frequency | Raw size    | Compressed         | Bandwidth | Savings             |
|-----------------------------------|-----------|-------------|--------------------|-----------|---------------------|
| VM tsdb API                       | Every 60s | ~4 MB       | ~400 KB (gzip)     | 6.7 KB/s  | 90%                 |
| VM metric_names_stats             | Every 60s | ~2.5 MB     | ~250 KB (gzip)     | 4.2 KB/s  | 90%                 |
| Peer /autotune/local (×9 peers)   | Every 30s | ~100 KB × 9 | ~15 KB × 9 (zstd)  | 4.5 KB/s  | 85%                 |
| **TOTAL (10-pod cluster)**        |           |             |                    | **~15 KB/s** | **98% vs uncompressed** |

Compare to the uncompressed estimate of ~776 KB/s — compression reduces autotune network traffic by 98% to just ~15 KB/s across all channels.

### Compression Metrics

Additional Prometheus metrics for compression observability:

```
metrics_governor_autotune_compression_bytes_total{direction="in|out",channel="peer|vm|endpoint"}
metrics_governor_autotune_compression_ratio{channel="peer|vm|endpoint"}
metrics_governor_autotune_compression_duration_seconds{channel="peer|vm"}
```

---

## Multi-Instance Coordination (Global State)

### Why sharding doesn't solve this

Even with sharding enabled (`internal/sharding/`), the consistent hash routes by label combination, not metric name. `http_requests_total{instance=A}` and `http_requests_total{instance=B}` may route to different nodes. The autotune score engine needs per-metric-name aggregates across all instances.

### Peer Discovery (reusing existing patterns)

Reuses the `sharding.Discovery` pattern (`internal/sharding/discovery.go:42-52`):

```go
type PeerDiscovery struct {
    resolver  sharding.Resolver  // reuse existing Resolver interface
    service   string             // headless service DNS name
    port      int                // autotune API port (same as stats addr)
    interval  time.Duration      // DNS refresh interval
    peers     []string           // discovered peer URLs
}
```

Config:

```yaml
autotune:
  cluster:
    enabled: true
    headless_service: "metrics-governor-headless.monitoring.svc"
    port: 9090  # same as stats addr
    refresh_interval: 30s
    compression: zstd  # peer-to-peer compression (zstd|gzip|snappy|none, default: zstd)
```

### Global Aggregation (designed for 6-10+ pods)

Naive approach (fetch all 50K scores from each of 10 peers) would be 10 × 10MB = 100MB per sync. Instead, we use a compact sync protocol:

#### Compact Peer Sync

Each instance exposes `/autotune/local` with two response modes:
- `?full=true` — all scores (for debugging/UI)
- Default — compact summary only:

```go
type CompactPeerState struct {
    InstanceID     string                  // pod name
    TotalMetrics   int                     // unique metrics on this instance
    TopScores      []MetricScore           // top-500 by composite score (actionable)
    Aggregates     PeerAggregates          // summary stats
    Timestamp      time.Time
}

type PeerAggregates struct {
    TotalCardinality   int64              // sum across all metrics
    ScoreDistribution  map[string]int     // healthy/watch/optimize/enforce/critical counts
    UnusedCount        int
    StaleCount         int
    TopGrowingMetrics  []string           // top-10 metric names by growth
    EstimatedSavings   int64              // total series saveable
}
```

Compact payload per peer: top-500 × ~200 bytes + aggregates ~1KB = **~100 KB** (vs 10 MB full).

```go
type GlobalAggregator struct {
    peers      *PeerDiscovery
    cache      *GlobalCache       // cached merged state
    cacheTTL   time.Duration      // scales with pod count: 30s + 5s per peer
    client     *http.Client
    topN       int                // top-N scores per peer to fetch (default: 500)
}

// Aggregate fetches compact state from all peers in parallel
func (g *GlobalAggregator) Aggregate(ctx context.Context) (*GlobalState, error) {
    // 1. Discover peers via headless service DNS
    // 2. GET /autotune/local from each peer (parallel, 3s timeout per peer)
    // 3. Merge:
    //    - Union of top-N scores; same metric on multiple peers: merge dimensions
    //    - Sum aggregate counts
    //    - Union top-growing metrics
    // 4. Cache with adaptive TTL (longer with more peers)
}

type GlobalState struct {
    Instances        int                   // peers discovered
    InstancesHealthy int                   // peers that responded
    TotalMetrics     int                   // sum of unique metrics across all peers
    MergedScores     []MetricScore         // merged top-N (up to topN × instances, deduped)
    GlobalAggregates PeerAggregates        // summed across all instances
    PerInstance      map[string]PeerAggregates
    AggregatedAt     time.Time
}
```

#### Scale Analysis (6-10 pods, 50K metrics each)

| Pods | Peer calls | Payload per peer | Total network per sync | Merge time | Cache TTL |
|------|------------|------------------|------------------------|------------|-----------|
| 3    | 2          | ~100 KB          | ~200 KB                | ~5ms       | 30s       |
| 6    | 5          | ~100 KB          | ~500 KB                | ~10ms      | 60s       |
| 10   | 9          | ~100 KB          | ~900 KB                | ~20ms      | 80s       |

Memory during merge: 10 × 500 scores × 200 bytes = **~1 MB peak** (vs 100 MB with full sync).

### Leader Election (optional)

By default, any pod can serve the global view (each does its own aggregation, cached). For large clusters, optional leader election avoids redundant aggregation:

```yaml
autotune:
  cluster:
    leader_election: true  # only leader aggregates; others proxy to leader
```

Uses Kubernetes lease-based leader election (via label annotation on the pod). Non-leaders serve `/autotune` by proxying to the leader's cached result.

### Endpoints

- `/autotune/local` — this instance's scores only (fast, no network)
- `/autotune/local?full=true` — full local scores (for debugging)
- `/autotune` — global aggregated view (compact sync, cached)
- `/autotune/ui` — HTML dashboard (uses global data)

### Single-Instance Mode

When `autotune.cluster.enabled` is false (default), `/autotune` returns the same as `/autotune/local`. No peer discovery, no network calls. Zero overhead for non-clustered deployments.

---

## State Cleanup (TTL-based eviction)

Metrics that disappear from traffic must be cleaned from autotune state to prevent unbounded memory growth.

### Cleanup Rules

```go
type CleanupConfig struct {
    MetricTTL       time.Duration `yaml:"metric_ttl"`        // default: 24h
    SnapshotRetention int          `yaml:"snapshot_retention"` // max snapshots to keep (default: 480)
    CleanupInterval time.Duration `yaml:"cleanup_interval"`   // default: 5m
}
```

**What gets cleaned:**

1. **Observer snapshots**: Ring buffer (bounded by `max_snapshots`), automatic eviction
2. **Per-metric scores**: If a metric hasn't appeared in any snapshot for `metric_ttl`, evict from score cache
3. **Applied autotune rules**: If the metric a rule was created for hasn't been seen in `metric_ttl`, retire the rule and remove it from config on next apply cycle
4. **Provider cache**: Stale entries evicted after 2× provider interval
5. **Persistent state**: On save, exclude metrics older than `metric_ttl`

Cleanup is logged:

```
msg="autotune state cleanup" evicted_metrics=12 evicted_rules=3 remaining_metrics=330
```

And exposed via metrics:

```
metrics_governor_autotune_state_metrics_total   330
metrics_governor_autotune_state_evictions_total  12
```

---

## HTML Dashboard (`/autotune/ui`)

A self-contained single-page HTML dashboard served from `/autotune/ui`. Follows the same pattern as the existing `tools/playground/index.html` — embedded in the binary, no external dependencies.

### Views

| View          | Content                                                                                                                     |
|---------------|-----------------------------------------------------------------------------------------------------------------------------|
| Overview      | Score distribution pie chart (healthy/watch/optimize/enforce/critical), total metrics, estimated savings, provider health    |
| Top Scores    | Sortable table of metrics with highest composite scores, dimension breakdown bars, suggested actions                         |
| Score Details | Click a metric → radar chart of 7 dimensions, history graph of score over time, action log                                   |
| Applied Rules | List of autotune-generated rules, when applied, series saved, rollback option                                                |
| Providers     | External provider status, cache hit rate, last fetch time, sample data                                                       |
| Config        | Current weights, thresholds, exemptions (read-only display)                                                                  |

### Implementation

- File: `internal/autotune/ui.go` — embedded HTML template (~300 lines)
- Uses `embed` directive to bundle HTML into binary
- JavaScript fetches `/autotune` (global) JSON endpoint
- Minimal CSS (no framework) — similar aesthetic to VictoriaMetrics VMUI
- Auto-refreshes every 30s
- Responsive (works on mobile for on-call checks)
- HTML estimated size: ~400 LOC (HTML+JS+CSS in single file)

---

## Architecture Overview

### New package: `internal/autotune/`

```
Engine (state machine)
  ├── Observer    — reads stats from Enforcer + Stats Collector (read-only)
  ├── Analyzer    — computes suggestions from observed data
  ├── Applier     — writes YAML, reloads config via Enforcer/Sampler
  └── HTTP handler — serves /autotune JSON endpoint
```

### State machine: `IDLE → LEARNING → SUGGESTING → APPLYING → MONITORING`

- **LEARNING**: Observer collects per-metric cardinality, rate, label patterns
- **SUGGESTING**: Analyzer computes P99-based limits with safety margin, processing rule suggestions
- **APPLYING**: Applier writes YAML + triggers reload (or just logs in suggest-only mode)
- **MONITORING**: Watches for violations after apply; rollback if threshold exceeded

**Continuous mode**: Stays in `LEARNING → SUGGESTING → APPLYING → MONITORING` loop indefinitely, re-evaluating every convergence interval.

---

## Pluggable External Data Source Architecture

The Observer layer isn't limited to reading internal stats — it supports pluggable external providers that fetch real-time cardinality and metrics data from external APIs. This creates a feedback loop where the backend's actual state informs autotune decisions.

### Provider Interface

Follows the existing `cardinality.Tracker` and `buffer.Exporter` interface patterns:

```go
// Provider fetches external metrics intelligence for autotune decisions.
// Implementations are registered via config and created by factory.
type Provider interface {
    // Name returns a human-readable identifier (e.g. "victoriametrics")
    Name() string

    // Fetch retrieves external cardinality/rate data. Called on Observer's tick.
    Fetch(ctx context.Context) (*ExternalData, error)

    // Close releases resources (HTTP clients, connections).
    Close() error
}

// ExternalData is the normalized output from any provider.
type ExternalData struct {
    Source          string                        // provider name
    Timestamp       time.Time
    MetricStats     map[string]ExternalMetricStat // metric name -> stats
    TopCardinality  []CardinalityEntry            // top-N by series count
    TotalSeries     int64
    TotalLabelPairs int64
}

type ExternalMetricStat struct {
    SeriesCount       int64              // active time series in backend
    LabelPairs        int64              // total label value combos
    TopLabels         []LabelCardinality // highest-cardinality labels
    IngestionRate     float64            // datapoints/sec (if available)
    QueryRequestCount int64             // times this metric was queried (0 = unused)
    LastQueryTime     time.Time          // last time metric was accessed (zero = never)
    GrowthRate        float64            // series/hour growth (computed from delta)
}

type CardinalityEntry struct {
    MetricName  string
    SeriesCount int64
    GrowthRate  float64 // positive = growing, negative = shrinking
}

type LabelCardinality struct {
    Name          string
    UniqueValues  int64
}
```

### Provider Factory (config-driven, like `cardinality.NewTracker`)

```go
// ProviderConfig configures one external provider instance.
type ProviderConfig struct {
    Type         string            `yaml:"type"`          // "victoriametrics" | future: "mimir", "prometheus"
    Endpoint     string            `yaml:"endpoint"`      // e.g. "http://vmselect:8481"
    Interval     time.Duration     `yaml:"interval"`      // poll interval (default: 60s)
    Timeout      time.Duration     `yaml:"timeout"`       // HTTP timeout (default: 10s)
    AuthBearer   string            `yaml:"auth_bearer"`   // optional bearer token
    AuthBasic    BasicAuth         `yaml:"auth_basic"`    // optional basic auth
    TLSEnabled   bool              `yaml:"tls_enabled"`
    TLSCAFile    string            `yaml:"tls_ca_file"`
    TLSSkipVerify bool            `yaml:"tls_skip_verify"`
    TopN         int               `yaml:"top_n"`         // top-N metrics to fetch (default: 100)
    Compression  string            `yaml:"compression"`   // request compression: "gzip" (default), "none"
    Streaming    bool              `yaml:"streaming"`     // use streaming JSON parser for large responses (default: true)
    Extra        map[string]string `yaml:"extra"`         // provider-specific settings
}

func NewProvider(cfg ProviderConfig) (Provider, error) {
    switch cfg.Type {
    case "victoriametrics":
        return newVMProvider(cfg)
    // Future: case "mimir": return newMimirProvider(cfg)
    // Future: case "prometheus": return newPrometheusProvider(cfg)
    default:
        return nil, fmt.Errorf("unknown autotune provider type: %q", cfg.Type)
    }
}
```

### First Module: VictoriaMetrics Cardinality Provider

**File**: `internal/autotune/provider_vm.go`

Uses the VictoriaMetrics TSDB status API:
- Single-node: `GET /api/v1/status/tsdb`
- Cluster: `GET /select/0/prometheus/api/v1/status/tsdb`

Query parameters: `?topN=100&date=2026-02-09`

The API returns:

```json
{
  "status": "success",
  "data": {
    "totalSeries": 485923,
    "totalLabelValuePairs": 2847591,
    "seriesCountByMetricName": [
      { "name": "http_requests_total", "value": 15230 },
      { "name": "node_cpu_seconds_total", "value": 8472 }
    ],
    "seriesCountByLabelName": [
      { "name": "instance", "value": 342891 },
      { "name": "pod", "value": 285123 }
    ],
    "seriesCountByLabelValuePair": [
      { "name": "job=prometheus", "value": 12345 }
    ],
    "labelValueCountByLabelName": [
      { "name": "instance", "value": 4523 },
      { "name": "pod", "value": 8921 }
    ]
  }
}
```

The VM provider calls two API endpoints for comprehensive intelligence:

#### Endpoint 1: `/api/v1/status/tsdb` — Cardinality snapshot

Request: `GET /api/v1/status/tsdb?topN={top_n}&date={today}&focusLabel={label}`

Response fields used:
- `seriesCountByMetricName` → per-metric active series count
- `seriesCountByLabelName` → which labels drive cardinality
- `seriesCountByFocusLabelValue` → drill down into specific labels
- `labelValueCountByLabelName` → unique values per label
- `totalSeries` / `totalLabelValuePairs` → aggregate cardinality

#### Endpoint 2: `/api/v1/status/metric_names_stats` — Usage & last access

Request: `GET /api/v1/status/metric_names_stats?limit=1000&older_than=720h`

Response fields used:
- `queryRequestsCount` → how often each metric is queried (0 = unused)
- `lastRequest` → timestamp of last query (stale = candidate for drop/downsample)

This enables Grafana Adaptive Metrics-style intelligence: identify metrics that are stored but never queried, metrics with declining usage, and metrics that haven't been accessed in N days.

```go
type vmProvider struct {
    client     *http.Client
    endpoint   string // base URL (e.g. http://vmselect:8481/select/0/prometheus)
    topN       int
    cluster    bool   // auto-detected from Extra["cluster"] or endpoint path
    fetchUsage bool   // whether to also call metric_names_stats (requires VM v1.108+)
}

func (v *vmProvider) Fetch(ctx context.Context) (*ExternalData, error) {
    // 1. Call /api/v1/status/tsdb for cardinality snapshot
    // 2. If fetchUsage, call /api/v1/status/metric_names_stats for usage data
    // 3. Merge into ExternalData with MetricUsage field populated
    // 4. Compute growth rates by comparing with previous Fetch result
}
```

### How Providers Feed Into Observer

The Observer merges internal + external data:

```go
type Observer struct {
    // ... existing fields ...
    providers   []Provider         // external data sources
    externalData map[string]*ExternalData  // latest per provider
}

// collectSnapshot merges internal stats + external provider data
func (o *Observer) collectSnapshot(enforcer, stats) Snapshot {
    snap := o.collectInternalStats(enforcer, stats)

    for _, p := range o.providers {
        ext, err := p.Fetch(ctx)
        if err != nil {
            // log warning, continue with internal data only
            continue
        }
        o.mergeExternalData(&snap, ext)
    }
    return snap
}
```

The Analyzer then uses the merged view — where external backend data (actual series stored) supplements internal proxy data (metrics flowing through). This is powerful because:
- **Internal**: "I see 5000 unique series per minute for metric X"
- **External (VM)**: "The backend has 50,000 total active series for metric X"
- **Combined insight**: "Cardinality is growing — series are accumulating faster than they expire"

### YAML Config Example

```yaml
autotune:
  enabled: true
  mode: apply
  learning_duration: 1h
  safety_margin: 1.3
  continuous: true
  output_path: /etc/metrics-governor/autotune-suggestions.yaml
  providers:
    - type: victoriametrics
      endpoint: http://vmselect.monitoring:8481/select/0/prometheus
      interval: 60s
      timeout: 10s
      auth_bearer: "${VM_AUTH_TOKEN}"
      top_n: 200
      extra:
        cluster: "true"   # uses cluster API path
    # Future providers:
    # - type: mimir
    #   endpoint: http://mimir-query:8080
    # - type: prometheus
    #   endpoint: http://prometheus:9090
```

---

## New Files

### `internal/autotune/provider.go` — Provider interface + factory

Core interface, ExternalData types, `NewProvider()` factory (see Plugin Architecture section above).

### `internal/autotune/provider_vm.go` — VictoriaMetrics provider

Implements `Provider` using VM's `/api/v1/status/tsdb` + `/api/v1/status/metric_names_stats` endpoints. Handles both single-node and cluster paths. Uses streaming JSON parsing for large responses. Requests gzip-compressed responses via `Accept-Encoding: gzip` (VM natively supports this). Uses HTTP client with TLS/auth from config.

### `internal/autotune/transport.go` — Shared HTTP transport + compression helpers

Pooled HTTP transport for peer sync and VM polling (reuses `internal/exporter/prw_exporter.go` patterns). Accept-Encoding negotiation (zstd preferred for peers, gzip for VM). Response decompression via existing `internal/compression/` package. Connection pooling scaled to cluster size (`maxPeers + 5` idle connections).

### `internal/autotune/engine.go` — Engine state machine

```go
type Engine struct {
    mu            sync.RWMutex
    state         State           // IDLE, LEARNING, SUGGESTING, APPLYING, MONITORING
    config        Config
    observer      *Observer
    analyzer      *Analyzer
    applier       *Applier
    suggestions   *Suggestions    // latest computed suggestions
    history       []ApplyRecord   // rollback stack
    startedAt     time.Time
    metrics       *engineMetrics
}

type Config struct {
    Enabled            bool              `yaml:"enabled"`
    Mode               string            `yaml:"mode"`               // "suggest" | "apply"
    LearningDuration   time.Duration     `yaml:"learning_duration"`  // e.g. 1h for bounded
    ConvergeInterval   time.Duration     `yaml:"converge_interval"`  // e.g. 15m for continuous
    SafetyMargin       float64           `yaml:"safety_margin"`      // e.g. 1.3 = 30% headroom
    Percentile         float64           `yaml:"percentile"`         // e.g. 0.99
    MinSamples         int               `yaml:"min_samples"`        // min observation cycles
    OutputPath         string            `yaml:"output_path"`        // where to write suggested YAML
    StatePath          string            `yaml:"state_path"`         // persist learning state across restarts
    MaxRollbacks       int               `yaml:"max_rollbacks"`      // safety limit
    ViolationThreshold float64           `yaml:"violation_threshold"`// rollback if >X% violations
    Continuous         bool              `yaml:"continuous"`         // enable continuous convergence
    Providers          []ProviderConfig  `yaml:"providers"`          // external data sources
    ExemptMetrics      []string          `yaml:"exempt_metrics"`     // regex patterns to never touch
    ExemptLabels       []string          `yaml:"exempt_labels"`      // labels to never suggest stripping
    StaleThreshold     time.Duration     `yaml:"stale_threshold"`    // how old = "stale" (default: 30d)
    UnusedQueryCount   int               `yaml:"unused_query_count"` // threshold for "unused" (default: 0)
    Weights            ScoreWeights      `yaml:"weights"`            // dimension weights
    Thresholds         ActionThresholds  `yaml:"thresholds"`         // score → action band boundaries
    MaxAutoApply       int               `yaml:"max_auto_apply"`     // max rules to apply per cycle (default: 10)
}

type ActionThresholds struct {
    Watch    float64 `yaml:"watch"`    // default: 0.20
    Optimize float64 `yaml:"optimize"` // default: 0.40
    Enforce  float64 `yaml:"enforce"`  // default: 0.60
    Critical float64 `yaml:"critical"` // default: 0.80
}

type State int
const (
    StateIdle State = iota
    StateLearning
    StateSuggesting
    StateApplying
    StateMonitoring
)
```

### `internal/autotune/observer.go` — Traffic observation (read-only)

Reads from existing interfaces without modifying them:
- `Enforcer.RuleStats()` — per-rule cardinality/rate/violations (already exposed via ServeHTTP)
- `stats.Collector.Snapshot()` — per-metric datapoints, label counts
- `Enforcer.GroupStats()` — group-by cardinality data

Uses streaming rolling statistics instead of raw snapshot history to keep memory bounded at O(metrics):

```go
type Observer struct {
    mu           sync.Mutex
    metrics      map[string]*MetricState   // per-metric rolling stats
    interval     time.Duration             // observation tick (e.g. 30s)
    providers    []Provider                // external data sources
    totalObs     int64                     // total observation ticks
}

type MetricState struct {
    Cardinality   RollingStats  // streaming P99/max/EMA for cardinality
    Rate          RollingStats  // streaming P99/max/EMA for datapoints rate
    LabelCount    int           // current number of labels
    TopLabels     map[string]int64  // label name -> distinct values (from last external fetch)
    ExternalData  *ExternalMetricStat // latest from provider (cached)
    LastSeen      time.Time
    SampleCount   int64
}

// RollingStats — O(1) memory per metric, streaming computation
type RollingStats struct {
    Count    int64
    P99      float64   // P2 algorithm streaming percentile
    Max      float64
    EMA      float64   // exponential moving average (α=0.1)
    Trend    float64   // EMA derivative (growth rate per observation)
    Last     float64   // most recent value
}
```

At 50K metrics × ~120 bytes per `MetricState` = **~6 MB** (vs ~3 GB for full snapshot history).

### `internal/autotune/score.go` — Score Engine

Core scoring system that evaluates every metric across 7 dimensions and computes composite scores.

### `internal/autotune/analyzer.go` — Action selection from scores

```go
type Analyzer struct {
    scorer    *ScoreEngine
    manual    *ManualRuleTracker  // knows which metrics are manually governed
    weights   ScoreWeights
    thresholds ActionThresholds
}

// Analyze scores all metrics and returns sorted suggestions
func (a *Analyzer) Analyze(snapshots []Snapshot, cfg Config) *AnalysisResult

type AnalysisResult struct {
    Scores      []MetricScore      // all metrics, sorted by composite score desc
    Suggestions *Suggestions       // filtered to actionable items above watch threshold
    Distribution map[string]int    // "healthy": 198, "watch": 67, ...
}
```

---

## Score Engine — Composite Metric Health Scoring

The core intelligence is a score engine that computes a composite health score (0.0–1.0) for every observed metric. The score drives graduated decision-making through configurable thresholds.

### Score Dimensions (0.0 = healthy, 1.0 = needs attention)

Each metric is evaluated across 7 orthogonal dimensions, each producing a sub-score:

| Dimension            | Signal Sources                                            | 0.0 (healthy)                                    | 1.0 (critical)                                         |
|----------------------|-----------------------------------------------------------|---------------------------------------------------|---------------------------------------------------------|
| Cardinality Pressure | Internal proxy P99 + VM seriesCountByMetricName           | Well within limit or low absolute count           | At/exceeding limit, or unbounded with high count        |
| Growth Trajectory    | Delta between VM tsdb snapshots over time                 | Stable or shrinking                               | Exponential growth (>5%/hr)                             |
| Usage Value          | VM metric_names_stats (queryRequestsCount, lastRequest)   | Heavily queried recently                          | 0 queries ever, or last queried >30d ago                |
| Label Efficiency     | VM labelValueCountByLabelName + internal label stats      | All labels low-cardinality and queried            | High-cardinality labels that are never used in queries  |
| Rate Impact          | Internal proxy datapoints/sec rate                        | At or below median                                | >10× median (top consumer)                              |
| Cost Weight          | VM totalSeries percentage                                 | <0.1% of total series                             | >5% of total (top cost contributor)                     |
| Rule Coverage        | Current manual rules config                               | Well-governed (matching limit + processing rules) | No rules at all (completely ungoverned)                 |

### Composite Score Computation

```go
type MetricScore struct {
    MetricName         string
    Dimensions         map[string]float64  // dimension name -> 0.0-1.0 sub-score
    CompositeScore     float64             // weighted average 0.0-1.0
    Confidence         float64             // how much data we have (0.0-1.0)
    TopContributor     string              // which dimension drove the score highest
    SuggestedActions   []ScoredAction      // what to do, ordered by priority
}

type ScoredAction struct {
    Action      string  // "drop", "sample", "downsample", "aggregate", "strip_labels", "add_limit", "tighten_limit", "increase_limit", "adjust_tenant_quota"
    Score       float64 // the threshold this action corresponds to
    Config      any     // action-specific config (rate, labels, limit values, etc.)
    Reason      string  // human-readable explanation
    SeriesSaved int64   // estimated cardinality reduction
}

// Weights are configurable per deployment (some care more about cost, others about safety)
type ScoreWeights struct {
    CardinalityPressure float64 `yaml:"cardinality_pressure"` // default: 0.25
    GrowthTrajectory    float64 `yaml:"growth_trajectory"`    // default: 0.20
    UsageValue          float64 `yaml:"usage_value"`          // default: 0.20
    LabelEfficiency     float64 `yaml:"label_efficiency"`     // default: 0.10
    RateImpact          float64 `yaml:"rate_impact"`          // default: 0.10
    CostWeight          float64 `yaml:"cost_weight"`          // default: 0.10
    RuleCoverage        float64 `yaml:"rule_coverage"`        // default: 0.05
}
```

The composite score = `Σ (weight_i × score_i)` normalized to 0.0–1.0. Weights are configurable in YAML so operators can prioritize what matters most to their deployment.

### Decision Thresholds → Actions

The score maps to graduated actions through configurable thresholds:

```
Score Range    │ Action Level     │ What Happens
───────────────┼──────────────────┼─────────────────────────────────────────────
0.00 – 0.20   │ HEALTHY          │ No action. Metric well-governed.
0.20 – 0.40   │ WATCH            │ Log insight. Suggest tighter manual rule.
               │                  │ Maybe increase existing limits if too tight.
0.40 – 0.60   │ OPTIMIZE         │ Suggest processing rule (sample/downsample).
               │                  │ Suggest strip_labels for unused high-card labels.
               │                  │ Suggest aggregate if pattern detected.
0.60 – 0.80   │ ENFORCE          │ Auto-apply processing rule + add limit.
               │                  │ Adjust tenant quota if tenant-scoped.
               │                  │ Add label_limits to constrain growth.
0.80 – 1.00   │ CRITICAL         │ Aggressive: drop if unused, strict limits if growing.
               │                  │ Emergency tenant quota reduction.
               │                  │ Generate alert-worthy log + metric spike.
```

Each threshold boundary is configurable:

```yaml
autotune:
  thresholds:
    watch: 0.20
    optimize: 0.40
    enforce: 0.60
    critical: 0.80
```

### Action Selection Logic

Within each threshold band, the top contributing dimension determines which action is most appropriate:

| Top Contributor        | OPTIMIZE Action          | ENFORCE Action                | CRITICAL Action                            |
|------------------------|--------------------------|-------------------------------|--------------------------------------------|
| Usage Value (unused)   | Suggest drop             | Apply drop                    | Apply drop + alert                         |
| Usage Value (stale)    | Suggest sample at 0.1    | Apply sample at 0.01          | Apply drop                                 |
| Cardinality Pressure   | Suggest limit at P99×1.3 | Apply limit at P99×1.1        | Apply limit at P99×1.0 (tight)             |
| Growth Trajectory      | Suggest limit + alert    | Apply limit + downsample      | Apply strict limit + sample                |
| Label Efficiency       | Suggest strip_labels     | Apply strip_labels            | Apply aggregate (collapse label)           |
| Rate Impact            | Suggest downsample       | Apply downsample              | Apply sample at aggressive rate            |
| Cost Weight            | Suggest aggregate        | Apply aggregate + limit       | Apply drop if unused, else limit           |
| Rule Coverage          | Suggest manual rule      | Generate YAML rule suggestion | Apply auto-rule + alert "needs governance" |

### Special: Tenant-Aware Scoring

If tenancy is enabled, per-tenant scores can drive tenant quota adjustments:
- Tenant at >80% of quota → score boost for that tenant's metrics
- Tenant consistently under-using quota → suggest increasing their headroom
- Cross-tenant: if one tenant's metrics dominate global cardinality → flag for quota review

### Special: Upward Adjustments (increasing limits)

Not all actions reduce — the score engine can also detect metrics that are too tightly constrained:
- If a manual limit causes >5% violation rate but metric is actively queried → suggest increasing limit
- If adaptive limiting is frequently triggering on a high-value metric → suggest raising the ceiling
- Score dimension "Rule Coverage" at 0.0 (well-governed) but "Cardinality Pressure" at 0.9 could mean the limit is too low, not that cardinality is too high

This is surfaced as a `ScoredAction` with action `"increase_limit"` or `"adjust_tenant_quota"` (upward).

### Algorithm Summary

```
For each observed metric:
  1. Collect sub-scores from all 7 dimensions
  2. Compute weighted composite score
  3. Check if metric matches any manual rule → if yes, flag as "governed"
  4. Check if metric matches any exemption → if yes, skip
  5. Determine threshold band (healthy/watch/optimize/enforce/critical)
  6. Select best action based on top contributing dimension
  7. Estimate series savings from action
  8. Rank all suggestions globally by composite score (descending)
  9. In apply mode: execute top-N actions that pass confidence threshold
```

---

## File Details: Remaining Components

### `internal/autotune/applier.go` — Config writing + reload

```go
type Applier struct {
    limitsReloader    func(cfg *limits.LimitsConfig)   // Enforcer.ReloadConfig
    processingReloader func(cfg *sampling.ProcessingConfig) error // Sampler.ReloadProcessingConfig
    outputPath        string
}

// WriteYAML writes suggested config to output_path with human-readable comments
func (a *Applier) WriteYAML(suggestions *Suggestions) error

// Apply writes YAML + triggers hot-reload via existing reload methods
func (a *Applier) Apply(suggestions *Suggestions) (*ApplyRecord, error)

// Rollback reverts to previous config from history stack
func (a *Applier) Rollback(record *ApplyRecord) error
```

### `internal/autotune/suggestions.go` — Suggestion types

```go
type Suggestions struct {
    Timestamp          time.Time
    LimitSuggestions   []LimitSuggestion
    ProcessingSuggestions []ProcessingSuggestion
    Confidence         float64
    SamplesCollected   int
}

type LimitSuggestion struct {
    MetricMatch          string   `json:"metric_match" yaml:"match"`
    SuggestedCardinality int64    `json:"suggested_cardinality" yaml:"max_cardinality"`
    SuggestedRate        int64    `json:"suggested_rate" yaml:"max_datapoints_rate"`
    ObservedP99Card      int64    `json:"observed_p99_cardinality"`
    ObservedP99Rate      float64  `json:"observed_p99_rate"`
    BackendSeriesCount   int64    `json:"backend_series_count,omitempty"` // from external provider
    GrowthRatePerHour    float64  `json:"growth_rate_per_hour,omitempty"`
    Confidence           float64  `json:"confidence"`
    Priority             int      `json:"priority"` // 0-100 score
    Reason               string   `json:"reason"`
}

type ProcessingSuggestion struct {
    Action       string   `json:"action" yaml:"action"`       // drop, sample, downsample, aggregate, strip_labels
    MetricMatch  string   `json:"metric_match" yaml:"input"`
    Labels       []string `json:"labels,omitempty" yaml:"labels,omitempty"`
    Rate         float64  `json:"rate,omitempty" yaml:"rate,omitempty"`
    Class        string   `json:"class"`                      // unused, stale, high_cardinality, high_rate, etc.
    QueryCount   int64    `json:"query_count,omitempty"`       // from VM usage stats (0 = unused)
    LastQueried  string   `json:"last_queried,omitempty"`      // ISO timestamp or "never"
    SeriesSaved  int64    `json:"series_saved,omitempty"`      // estimated reduction
    Confidence   float64  `json:"confidence"`
    Priority     int      `json:"priority"`                    // 0-100 score
    Reason       string   `json:"reason"`
}

type ApplyRecord struct {
    Timestamp      time.Time
    PreviousLimits *limits.LimitsConfig
    PreviousProc   *sampling.ProcessingConfig
    Applied        *Suggestions
}
```

### `internal/autotune/http.go` — `/autotune` JSON endpoint

```go
func (e *Engine) ServeHTTP(w http.ResponseWriter, r *http.Request)
```

Returns JSON:

```json
{
  "state": "monitoring",
  "started_at": "2026-02-09T12:00:00Z",
  "learning_progress": 0.85,
  "providers": [
    { "name": "victoriametrics", "status": "healthy", "last_fetch": "2026-02-09T12:05:00Z",
      "total_series": 485923, "cache_hit_rate": 0.92 }
  ],
  "insights": {
    "total_metrics_observed": 342,
    "score_distribution": { "healthy": 198, "watch": 67, "optimize": 45, "enforce": 23, "critical": 9 },
    "estimated_savings": { "series_reduction": 48230, "percentage": 9.9 }
  },
  "top_scores": [
    {
      "metric": "telemetry_debug_events_total",
      "composite_score": 0.94,
      "top_contributor": "usage_value",
      "dimensions": { "cardinality_pressure": 0.3, "usage_value": 1.0, "growth": 0.5 },
      "suggested_action": { "action": "drop", "reason": "0 queries ever, 8472 series", "series_saved": 8472 }
    },
    {
      "metric": "http_requests_total",
      "composite_score": 0.72,
      "top_contributor": "growth_trajectory",
      "dimensions": { "cardinality_pressure": 0.7, "growth": 0.9, "usage_value": 0.1 },
      "suggested_action": { "action": "add_limit", "max_cardinality": 20000, "series_saved": 0 }
    }
  ],
  "applied": { "timestamp": "...", "rules_applied": 8, "series_saved": 32150 },
  "upward_adjustments": [
    { "metric": "api_latency_seconds", "action": "increase_limit", "from": 5000, "to": 8000,
      "reason": "5.2% violation rate on actively-queried metric" }
  ],
  "config": { "mode": "apply", "weights": {}, "thresholds": {} }
}
```

### `internal/autotune/metrics.go` — Prometheus metrics

```go
type engineMetrics struct {
    // Engine state
    state              prometheus.Gauge     // current state enum
    learningProgress   prometheus.Gauge     // 0.0-1.0

    // Suggestion counts
    suggestionsTotal   *prometheus.CounterVec // total suggestions by class (unused, stale, high_card, ...)
    appliedTotal       prometheus.Counter     // total configs applied
    rollbacksTotal     prometheus.Counter     // total rollbacks

    // Observation
    observationCount   prometheus.Gauge     // snapshots in buffer
    confidenceScore    prometheus.Gauge     // latest confidence
    violationsAfterApply prometheus.Gauge   // violations in monitoring period

    // Intelligence insights
    unusedMetricsCount  prometheus.Gauge     // metrics with 0 queries (from VM)
    staleMetricsCount   prometheus.Gauge     // metrics last queried >30d ago
    topGrowingRate      *prometheus.GaugeVec // per-metric growth rate (top N)
    estimatedSavings    prometheus.Gauge     // series reduction if all suggestions applied
    providerFetchErrors *prometheus.CounterVec // per-provider errors
    providerLastFetch   *prometheus.GaugeVec   // per-provider last successful fetch timestamp

    // Per-metric details (top N only to avoid cardinality explosion)
    suggestedCardinality *prometheus.GaugeVec // per-metric suggested limits
    suggestedRate        *prometheus.GaugeVec // per-metric suggested rates
}
```

All metrics prefixed with `metrics_governor_autotune_`. Key metrics for dashboards:
- `_score_distribution{band="healthy|watch|optimize|enforce|critical"}` — how many metrics in each band
- `_composite_score_histogram` — histogram of all metric scores
- `_unused_metrics_count` — how many ingested metrics are never queried
- `_stale_metrics_count` — metrics not queried in >30 days
- `_estimated_savings_series` — potential series reduction
- `_actions_total{action="drop|sample|add_limit|increase_limit|..."}` — by action type
- `_provider_fetch_errors_total{provider="victoriametrics"}` — provider health
- `_provider_cache_hit_ratio{provider="victoriametrics"}` — cache effectiveness
- `_upward_adjustments_total` — limits increased (shows system is also loosening, not just tightening)

### `internal/autotune/yaml_writer.go` — YAML generation with comments

Generates human-readable YAML with rich context comments:

```yaml
# Auto-generated by metrics-governor autotune
# Generated: 2026-02-09T12:00:00Z
# Based on 120 observation samples over 1h + VictoriaMetrics backend data
# Safety margin: 1.3x (30% headroom above P99)
# Estimated savings: 48,230 series (9.9% reduction)

# ─── LIMITS RULES ─────────────────────────────────────────
rules:
  - name: "autotune_http_requests_total"
    match: "http_requests_total"
    # [Priority: 95] Top growing metric (+2.3%/hr)
    # Backend series: 15,230 | Proxy P99 cardinality: 2,847
    # Confidence: 0.95 (120/100 samples)
    max_cardinality: 4000
    max_datapoints_rate: 20000
    action: "drop"

# ─── PROCESSING RULES ────────────────────────────────────
processing_rules:
  - name: "autotune_drop_unused_telemetry_debug"
    input: "telemetry_debug_.*"
    action: "drop"
    # [Priority: 90] Class: unused — 0 queries, last access: never
    # Backend series: 8,472 — dropping saves 8,472 series

  - name: "autotune_strip_pod_from_node_metrics"
    input: "node_.*"
    action: "strip_labels"
    labels: ["pod"]
    # [Priority: 78] Class: high_cardinality_label
    # Label "pod" has 4,523 unique values, drives 82% of cardinality
    # Estimated savings: 12,891 series

  - name: "autotune_downsample_high_rate_api"
    input: "api_request_duration_seconds"
    action: "downsample"
    rate: 0.1
    # [Priority: 65] Class: high_rate — 45,230 dp/s (12x median)
    # Last queried: 2026-02-08 (active, so downsample not drop)
```

---

## Existing Files to Modify

### 1. `internal/config/config.go` (lines 24-330)

Add new fields to Config struct:

```go
// Autotune
AutotuneEnabled           bool          // --autotune-enabled
AutotuneMode              string        // --autotune-mode (suggest|apply)
AutotuneLearningDuration  time.Duration // --autotune-learning-duration
AutotuneConvergeInterval  time.Duration // --autotune-converge-interval
AutotuneSafetyMargin      float64       // --autotune-safety-margin
AutotunePercentile        float64       // --autotune-percentile
AutotuneMinSamples        int           // --autotune-min-samples
AutotuneOutputPath        string        // --autotune-output-path
AutotuneContinuous        bool          // --autotune-continuous
```

### 2. `internal/config/flags.go`

Add CLI flag registrations for all autotune flags with sensible defaults:
- `--autotune-enabled` (default: false)
- `--autotune-mode` (default: "suggest")
- `--autotune-learning-duration` (default: 1h)
- `--autotune-safety-margin` (default: 1.3)
- etc.

### 3. `internal/config/yaml.go`

Add YAML mapping for autotune section under `autotune:` key.

### 4. `internal/limits/enforcer.go` (lines 41-83)

Add read-only accessor methods needed by Observer:

```go
// Snapshot returns a read-only copy of current rule statistics
func (e *Enforcer) RuleStatsSnapshot() map[string]RuleStatsEntry

// GroupStatsSnapshot returns group-by cardinality data
func (e *Enforcer) GroupStatsSnapshot() map[string]map[string]int64
```

These are read-only — they lock `mu`, copy data, unlock. No mutation.

### 5. `internal/stats/stats.go` (lines 35-83)

Add read-only snapshot method:

```go
// MetricStatsSnapshot returns a copy of per-metric statistics
func (c *Collector) MetricStatsSnapshot() map[string]MetricStats
```

### 6. `cmd/metrics-governor/main.go`

- Wire `autotune.Engine` creation after Enforcer + Sampler + Stats Collector are initialized (~line 650)
- Register `/autotune` HTTP handler on stats mux
- Start engine goroutine via `engine.Start(ctx)`
- Add autotune section to SIGHUP handler (reload autotune config)
- Register autotune readiness check with `health.Checker`

---

## Integration Points

### Reading data (Observer)

Observer calls read-only snapshot methods on:
- `*limits.Enforcer` → `RuleStatsSnapshot()`, `GroupStatsSnapshot()`
- `*stats.Collector` → `MetricStatsSnapshot()`

### Applying config (Applier)

Applier calls existing reload methods:
- `limitsEnforcer.ReloadConfig(newConfig)` — already used by SIGHUP handler (`main.go:816`)
- `sampler.ReloadProcessingConfig(newConfig)` — already used by SIGHUP handler (`main.go:843`)

### YAML output

Writes to `AutotuneOutputPath` (default: `/tmp/autotune-suggestions.yaml`). The file uses standard limits/processing YAML format so it can be directly used as `--limits-config` or `--processing-config`.

### Existing patterns reused

- `internal/compression/compression.go` — Pooled gzip/zstd/snappy compression (used for peer sync, VM responses, state persistence)
- `internal/exporter/prw_exporter.go` — HTTP transport config pattern (DisableCompression, connection pooling, HTTP/2)
- `internal/exporter/batchtuner.go` — AIMD auto-tuning pattern (grow/shrink factors)
- `internal/limits/log_aggregator.go` — Log batching pattern for similar messages
- `internal/ruleactivity/activity.go` — Activity tracking with timestamps
- `internal/health/health.go` — `RegisterReadiness()` for health integration
- `cmd/metrics-governor/main.go:669-680` — Accept-Encoding negotiation pattern for stats endpoint (reused for `/autotune`)

---

## Test Plan

### Unit tests (~10 files)

| File                          | Tests                                                                                          |
|-------------------------------|------------------------------------------------------------------------------------------------|
| `provider_test.go`            | Factory creation, unknown type error, config validation                                        |
| `provider_vm_test.go`         | VM tsdb + metric_names_stats parsing, cluster/single paths, auth, mock HTTP server             |
| `provider_cache_test.go`      | TTL expiry, stale-on-error serving, concurrent access, cache invalidation                      |
| `engine_test.go`              | State transitions, Start/Stop, continuous mode, bounded mode, state persistence                |
| `observer_test.go`            | Snapshot collection, ring buffer, internal+external merge, provider failure fallback            |
| `score_test.go`               | All 7 dimension sub-scores, composite weighting, edge cases (missing data, single dimension)   |
| `score_thresholds_test.go`    | Band classification, action selection by top contributor, upward adjustments                    |
| `analyzer_test.go`            | Full analysis pipeline, distribution counting, suggestion filtering                            |
| `analyzer_limits_test.go`     | Limit suggestion with backend data, growth rate, increase_limit detection                      |
| `analyzer_processing_test.go` | Unused/stale/high-card/high-rate detection from combined signals                               |
| `manual_rules_test.go`        | Manual rule detection, exemption patterns, conflict resolution, autotune rule retirement       |
| `applier_test.go`             | YAML write, reload trigger, rollback, non-destructive append                                   |
| `state_test.go`               | Persist/restore learning state, resume after restart                                           |
| `cleanup_test.go`             | TTL eviction, rule retirement for disappeared metrics, bounded memory                          |
| `peers_test.go`               | DNS discovery, peer list updates, mock resolver                                                |
| `transport_test.go`           | Compression negotiation, pooled encoder reuse, connection pooling, streaming parse             |
| `global_test.go`              | Multi-instance score merging, cache TTL, peer failure tolerance, compressed exchange           |
| `suggestions_test.go`         | Serialization, JSON marshaling, priority sorting                                               |
| `http_test.go`                | `/autotune`, `/autotune/local`, `/autotune/ui` endpoints                                       |
| `metrics_test.go`             | Prometheus metric registration, score distribution, cleanup counts                             |
| `yaml_writer_test.go`         | Rich comments, class annotations, roundtrip parsing                                            |

### Integration test

- `autotune_integration_test.go`: Full cycle — create Enforcer with mock traffic, start Engine, wait for learning, verify suggestions, verify apply, verify rollback on violations.

### Verification steps

1. `go build ./...` — compiles
2. `go test -race -count=1 ./internal/autotune/...` — unit tests pass with race detector
3. `go test -race -count=1 ./internal/limits/... ./internal/stats/...` — existing tests still pass
4. `go test -race -count=1 ./...` — full suite passes
5. Manual: start governor with `--autotune-enabled --autotune-mode=suggest --autotune-learning-duration=1m`, send traffic, check `/autotune` endpoint and log output

---

## Impact Estimation

### Lines of Code

| Component                     | New LOC (impl) | New LOC (tests) | Modified LOC |
|-------------------------------|----------------|-----------------|--------------|
| autotune/provider.go          | ~120           | ~60             | —            |
| autotune/provider_vm.go       | ~280           | ~320            | —            |
| autotune/provider_cache.go    | ~100           | ~120            | —            |
| autotune/engine.go            | ~300           | ~380            | —            |
| autotune/observer.go          | ~180           | ~230            | —            |
| autotune/score.go             | ~200           | ~300            | —            |
| autotune/analyzer.go          | ~140           | ~180            | —            |
| autotune/analyzer_limits.go   | ~160           | ~220            | —            |
| autotune/analyzer_processing.go | ~180         | ~250            | —            |
| autotune/manual_rules.go      | ~120           | ~180            | —            |
| autotune/applier.go           | ~150           | ~210            | —            |
| autotune/suggestions.go       | ~110           | ~90             | —            |
| autotune/state.go             | ~120           | ~140            | —            |
| autotune/cleanup.go           | ~80            | ~120            | —            |
| autotune/peers.go             | ~150           | ~200            | —            |
| autotune/global.go            | ~180           | ~220            | —            |
| autotune/transport.go         | ~90            | ~80             | —            |
| autotune/http.go              | ~150           | ~200            | —            |
| autotune/ui.go                | ~50 (+400 HTML)| ~60             | —            |
| autotune/metrics.go           | ~150           | ~120            | —            |
| autotune/yaml_writer.go       | ~130           | ~150            | —            |
| autotune/integration_test.go  | —              | ~450            | —            |
| Config (flags/yaml/config.go) | —              | —               | ~120         |
| Enforcer (snapshot methods)   | —              | —               | ~40          |
| Stats Collector (snapshot)    | —              | —               | ~20          |
| main.go (wiring)              | —              | —               | ~50          |
| **TOTAL**                     | **~3,260 (+400 HTML)** | **~4,300** | **~230**   |

### Relative to Codebase

| Metric                     | Value                                     |
|----------------------------|-------------------------------------------|
| Current internal/ impl LOC | 33,770                                    |
| Current internal/ test LOC | 97,610                                    |
| New impl LOC added         | ~3,260 + 400 HTML (9.7% increase + UI)    |
| New test LOC added         | ~4,300 (4.4% increase)                    |
| Existing files modified    | 5 files, ~230 lines changed               |
| New files created          | 22 source + 1 HTML + 22 test = 45 files   |
| New package                | 1 (`internal/autotune/`)                  |

---

## Resource Impact Analysis (100K dps, 50K unique metrics per instance)

**Critical**: Autotune runs entirely off the hot path. It never touches the request processing pipeline (receiver → buffer → queue → exporter). All work happens on background timer goroutines.

### Memory Impact

| Component                 | Formula                                                            | Estimate              |
|---------------------------|--------------------------------------------------------------------|-----------------------|
| Observer rolling stats    | 50K metrics × ~120 bytes (name ref + P99/max/trend/rate/last_seen) | 6 MB                  |
| Score cache               | 50K metrics × ~200 bytes (7 dimensions + composite + actions)      | 10 MB                 |
| Provider L1 cache         | VM tsdb response (~50K entries × 80 bytes)                         | 4 MB                  |
| Provider L1 cache         | VM metric_names_stats (~50K entries × 50 bytes)                    | 2.5 MB                |
| Analysis results cache    | Top-1000 suggestions × ~500 bytes                                  | 0.5 MB                |
| Peer aggregation temp     | 10 peers × 100 KB compact (during merge)                           | ~1 MB peak, 0 at rest |
| HTML UI                   | Embedded template                                                  | ~15 KB                |
| Persistent state file     | Compressed JSON on disk                                            | ~2 MB disk (not mem)  |
| **TOTAL (steady state)**  |                                                                    | **~23 MB**            |
| **TOTAL (during merge)**  |                                                                    | **~53 MB peak**       |

For context: The main pipeline at 100K dps with 50K metrics typically uses 200-500 MB depending on buffer/queue config. Autotune adds ~5-10% memory overhead at steady state.

**Design note**: The Observer uses rolling statistics (P99/max/EMA trend) per metric instead of raw snapshot history. This keeps memory bounded at O(metrics) instead of O(metrics × snapshots):

```go
type RollingStats struct {
    Count        int64     // total observations
    P99          float64   // approximated via streaming percentile (P2 algorithm)
    Max          float64   // all-time max in window
    EMA          float64   // exponential moving average (α=0.1)
    Trend        float64   // first derivative of EMA (growth rate)
    LastValue    float64   // most recent observation
    LastSeen     time.Time // for TTL cleanup
}
```

This is ~96 bytes per metric, not ~12.5 MB per full snapshot.

### CPU Impact

| Operation               | Frequency              | Cost                                               | CPU % (of one core)              |
|-------------------------|------------------------|-----------------------------------------------------|----------------------------------|
| Observer tick           | Every 30s              | Read stats collector O(50K) + update rolling stats  | ~15ms → 0.05%                    |
| Provider fetch          | Every 60s              | HTTP GET + JSON parse 50K entries                   | ~50-200ms → 0.2% (mostly I/O)   |
| Score computation       | Every 15m (converge)   | 50K metrics × 7 dimensions                          | ~10-30ms → <0.01%               |
| Cleanup sweep           | Every 5m               | Scan 50K metrics for TTL expiry                     | ~5ms → <0.01%                   |
| Global aggregation      | Every 30s (if cluster) | 3 HTTP calls + merge 150K entries                   | ~50-100ms → 0.15%               |
| HTTP /autotune          | On demand              | Serve cached JSON                                   | ~1ms → negligible               |
| **TOTAL (single)**      |                        |                                                     | **~0.3% of one CPU core**       |
| **TOTAL (cluster)**     |                        |                                                     | **~0.5% of one CPU core**       |

For context: The main pipeline at 100K dps uses 50-200% of one core. Autotune adds <1% CPU overhead.

### Network Impact (cluster mode, with compression)

| Call                              | Frequency | Raw size    | Compressed         | Bandwidth |
|-----------------------------------|-----------|-------------|--------------------|-----------|
| VM tsdb API                       | Every 60s | ~4 MB       | ~400 KB (gzip)     | 6.7 KB/s  |
| VM metric_names_stats             | Every 60s | ~2.5 MB     | ~250 KB (gzip)     | 4.2 KB/s  |
| Peer /autotune/local (×9 peers)   | Every 30s | ~100 KB × 9 | ~15 KB × 9 (zstd)  | 4.5 KB/s  |
| **TOTAL (10-pod cluster)**        |           |             |                    | **~15 KB/s** |

Compression reduces network overhead by 98% — from ~776 KB/s (uncompressed) to ~15 KB/s.

### Hot Path Impact: ZERO

The autotune system is completely decoupled from the data pipeline:
- Observer reads stats via read-only snapshot methods (acquires a read lock for ~microseconds)
- No per-request processing, no per-datapoint overhead
- Even if the score engine hangs or provider times out, zero impact on metric flow
- The stats collector already maintains per-metric counters; Observer just reads them periodically

### Scaling Considerations

| Metric count | Memory (steady) | Score compute time |
|--------------|-----------------|---------------------|
| 10K metrics  | ~5 MB           | ~3ms                |
| 50K metrics  | ~23 MB          | ~15ms               |
| 100K metrics | ~46 MB          | ~30ms               |
| 500K metrics | ~230 MB         | ~150ms              |

At 500K unique metrics (extreme), memory becomes significant. Mitigation: top-N scoring mode — only maintain full scores for top 10K by estimated impact, use lightweight tracking for the rest.

---

## Complexity Assessment

| Risk                            | Level  | Mitigation                                                                                  |
|---------------------------------|--------|---------------------------------------------------------------------------------------------|
| Regression to existing pipeline | Low    | Observer is read-only; Applier uses existing reload methods                                 |
| Performance impact on hot path  | None   | Observer runs on its own ticker goroutine, never in request path                            |
| Config surface area growth      | Medium | ~15 new YAML keys, all optional with sensible defaults                                     |
| External API dependency         | Low    | Provider failures are non-fatal — cache serves stale data, Observer falls back to internal  |
| Multi-instance coordination     | Low    | Optional, peer failures are tolerated, falls back to local-only view                        |
| State memory growth             | Low    | TTL-based cleanup with configurable retention, bounded ring buffer                          |
| Embedded HTML size              | Low    | ~400 LOC single-file HTML, no frameworks, adds ~15KB to binary                              |
| Rollback safety                 | Low    | Capped at max_rollbacks, reverts to known-good config                                       |
| Continuous mode runaway         | Low    | Safety margin + violation threshold prevent tightening past safe limits                     |

### Comparable feature sizes in this codebase

| Feature             | Impl LOC | Reference                         |
|---------------------|----------|-----------------------------------|
| Batch tuner (AIMD)  | 220      | `internal/exporter/batchtuner.go` |
| Limits enforcer     | 2,419    | `internal/limits/`                |
| Dead rule scanner   | ~150     | `internal/limits/dead_rules.go`   |
| Autotune (proposed) | ~2,620   | Large feature                     |

The autotune package is roughly 30% larger than the limits enforcer. It breaks down as: provider subsystem (~530 LOC with cache), score engine + analyzer (~800 LOC), engine + applier + state (~670 LOC), multi-instance coordination + transport (~420 LOC incl compression), HTTP/UI/metrics/YAML (~860 LOC incl HTML). Future providers (Mimir, Prometheus) would each add ~200-250 LOC following the same pattern.

---

## Architecture Diagram Update

The current SVG (`docs/images/architecture.svg`, 376 lines) uses a 4-phase layout:

```
Sources → [ INGEST | PROCESS | BUFFER/QUEUE | EXPORT ] → Backends
               OTLP lane (cyan)
               Shared features (gray)
               PRW lane (green)
                                                     Self-Monitoring (dashed box, right)
```

### Where Autotune fits

Autotune is a feedback loop — it observes the PROCESS phase's stats and feeds suggested config back into PROCESS and BUFFER/QUEUE. On the diagram, it should appear as:

```
                                                    ┌─────────────────┐
                                                    │  External APIs   │
                                                    │ (VM, Mimir, ...) │
                                                    └────────┬────────┘
                                                             │ /api/v1/status/tsdb
                                                             ▼
┌─────────────────── metrics-governor ──────────────────────────────────┐
│                                                                       │
│  INGEST → PROCESS → BUFFER/QUEUE → EXPORT                            │
│              ↑ ↓                                                      │
│         ┌─────────────────────────────────┐                           │
│         │          AUTOTUNE               │                           │
│         │  [Providers] → observe →        │                           │
│         │    analyze → suggest → apply    │                           │
│         └─────────────────────────────────┘                           │
│                                                                       │
└───────────────────────────────────────────────────────────────────────┘
                                                      Self-Monitoring ──┘
                                                      /autotune + /metrics
```

### SVG Changes

Add below the existing governor phases (y≈430-480):

1. **Autotune box** (x=350, y=430, w=280, h=50) — "AUTOTUNE" label with indigo gradient
   - Subtitle: `observe · analyze · suggest · apply`
   - Dashed border (like Self-Monitoring) to indicate optional/advisory
2. **External APIs box** (x=980, y=430, w=175, h=50) — new column-right box
   - Label: "External APIs"
   - Subtitle: `VictoriaMetrics · Mimir · ...`
   - Dashed border, neutral gray gradient
3. **Feedback arrows**:
   - Dashed upward arrow from AUTOTUNE to PROCESS box (suggests limits)
   - Dashed upward arrow from AUTOTUNE to BUFFER/QUEUE box (suggests processing rules)
   - Downward arrow from PROCESS to AUTOTUNE (observes stats)
   - Horizontal dashed arrow from External APIs → AUTOTUNE (provider data)
4. Arrow to Self-Monitoring: Annotation showing `/autotune` endpoint alongside `/metrics`
5. Viewport height increase: 550 → 600 to accommodate the new row
6. Footer tagline update: Add "autotune" to the tagline text

Estimated SVG diff: ~45 new lines, ~5 modified lines.

---

## Execution Order

### Phase 1: Foundation

1. Create `internal/autotune/` package with core types (`suggestions.go`, config, score types)
2. Implement score engine with 7 dimensions + composite weighting (`score.go`)
3. Implement manual rule detection + exemption matching (`manual_rules.go`)

### Phase 2: Data Collection

4. Implement Provider interface + factory + cache wrapper (`provider.go`, `provider_cache.go`)
5. Implement VictoriaMetrics provider with dual APIs (`provider_vm.go`)
6. Add read-only snapshot methods to Enforcer and Stats Collector
7. Implement Observer with internal + external data merge (`observer.go`)

### Phase 3: Intelligence

8. Implement Analyzer with score-driven action selection (`analyzer.go`, `analyzer_limits.go`, `analyzer_processing.go`)
9. Implement Applier with non-destructive append + rollback (`applier.go`)
10. Implement state persistence + cleanup (`state.go`, `cleanup.go`)

### Phase 4: Engine

11. Implement Engine state machine (ties all components)
12. Add Config/flags/YAML support (including `providers:`, `exempt_*`, `state_path`, `weights`, `thresholds`)
13. Wire into `main.go`

### Phase 5: Multi-Instance + Observability

14. Implement peer discovery + global aggregation (`peers.go`, `global.go`)
15. Add HTTP handlers (`/autotune`, `/autotune/local`, `/autotune/ui`)
16. Add Prometheus metrics
17. Build HTML dashboard (`ui.go` + embedded HTML)

### Phase 6: Testing + Docs

18. Write tests (unit + integration, mock HTTP server for VM + peers)
19. Update docs (`configuration.md`, README autotune section)
20. Update architecture SVG with autotune feedback loop + external providers
