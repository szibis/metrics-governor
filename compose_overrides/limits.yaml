# Limits enforcement testing configuration - moderate load (half of perf.yaml)
# Usage: docker compose -f docker-compose.yaml -f compose_overrides/limits.yaml up -d --build
#
# This override enables real limits enforcement with spike scenarios at moderate load:
# - Generator: spike scenarios enabled, ~50% of perf.yaml throughput
# - metrics-governor: limits-dry-run=false (real enforcement)
# - Tight limits calibrated to trigger violations
# - Verifier: threshold lowered (data will be intentionally dropped)
#
# Expected timeline (continuous, repeating):
# - Warm-up (0-30s): Cardinality building, normal limits start triggering
# - Steady state: Normal traffic continuously exceeds tight limits
# - Random spikes: Every 30-120s, 1000 series spike, gets hard-dropped
# - Mistake cycles: After random delay, 100 series/sec for 2min, adaptive drops
# - Recovery: After each event ends, violations clear within 60s

services:
  metrics-generator:
    environment:
      - OTLP_ENDPOINT=otel-collector:4317
      # Use low-memory temporality: delta for counters/histograms (reduces memory)
      - OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE=lowmemory
      - METRICS_INTERVAL=100ms
      - SERVICES=payment-api,order-api,inventory-api,user-api,auth-api,legacy-app,billing-api,notification-api
      - ENVIRONMENTS=prod,staging,dev,qa
      - ENABLE_EDGE_CASES=true
      - ENABLE_HIGH_CARDINALITY=true
      - ENABLE_BURST_TRAFFIC=true
      - ENABLE_DIVERSE_METRICS=true
      - HIGH_CARDINALITY_COUNT=250   # Half of perf (500)
      - DIVERSE_METRIC_COUNT=150     # Half of perf (300)
      - BURST_SIZE=2500              # Half of perf (5000)
      - BURST_INTERVAL_SEC=20
      - STATS_INTERVAL_SEC=5
      - TARGET_METRICS_PER_SEC=50000   # Half of perf (100000)
      - TARGET_DATAPOINTS_PER_SEC=100000  # Half of perf (200000)
      - METRICS_PORT=9091
      # Spike scenario configuration (enabled for limits testing)
      - ENABLE_SPIKE_SCENARIOS=true
      - SPIKE_MODE=realistic
      - SPIKE_CARDINALITY=500        # Half of default (1000)
      - SPIKE_DURATION_SEC=30
      - SPIKE_INTERVAL_MIN_SEC=15
      - SPIKE_INTERVAL_MAX_SEC=45
      - MISTAKE_DELAY_SEC=20
      - MISTAKE_DURATION_SEC=90
      - MISTAKE_CARDINALITY_RATE=50
    deploy:
      resources:
        limits:
          # OTel SDK uses cumulative aggregation - stores all unique series in memory
          # With spike scenarios enabled, memory grows significantly
          memory: 4G
          cpus: "1.5"

  metrics-governor:
    command:
      - "-exporter-endpoint=http://victoriametrics:8428/opentelemetry/v1/metrics"
      - "-exporter-protocol=http"
      - "-exporter-insecure=true"
      - "-exporter-compression=zstd"
      - "-stats-labels=service,env"
      - "-limits-config=/etc/metrics-governor/limits-testing.yaml"
      - "-limits-dry-run=false"      # Real enforcement!
      - "-flush-interval=75ms"
      - "-batch-size=350"
      - "-max-batch-bytes=8388608"   # 8MB
      - "-buffer-size=25000"
      # Performance tuning
      - "-export-concurrency=32"     # Half of perf (64)
      - "-string-interning=true"
      - "-intern-max-value-length=64"
      # Disk queue - persistent failover
      - "-queue-enabled=true"
      - "-queue-type=disk"
      - "-queue-path=/data/queue"
      - "-queue-max-size=25000"
      - "-queue-max-bytes=5368709120"      # 5GB
      - "-queue-retry-interval=3s"
      - "-queue-full-behavior=drop_oldest"
      - "-queue-adaptive-enabled=true"
      - "-queue-inmemory-blocks=384"
      - "-queue-chunk-size=201326592"      # 192MB
      - "-queue-meta-sync=750ms"
      - "-queue-stale-flush=3s"
      # Queue resilience
      - "-queue-backoff-enabled=true"
      - "-queue-backoff-multiplier=1.75"
      - "-queue-circuit-breaker-enabled=true"
      - "-queue-circuit-breaker-threshold=15"
      - "-queue-circuit-breaker-reset-timeout=20s"
      # Rule cache
      - "-rule-cache-max-size=35000"
      # Hybrid cardinality tracking (auto Bloomâ†’HLL)
      - "-cardinality-mode=hybrid"
      - "-cardinality-hll-threshold=5000"
      # Structured logging
      - "-stats-log-interval=10s"
      - "-limits-log-interval=10s"
      # Memory limit
      - "-memory-limit-ratio=0.85"
    volumes:
      - mg-queue-limits-data:/data/queue
      - ./examples/limits-testing.yaml:/etc/metrics-governor/limits-testing.yaml:ro
    deploy:
      resources:
        limits:
          memory: 3G
          cpus: "1.5"

  victoriametrics:
    deploy:
      resources:
        limits:
          memory: 8G                 # Half of perf (16G)
          cpus: "2"
    command:
      - "--storageDataPath=/victoria-metrics-data"
      - "--httpListenAddr=:8428"
      - "--retentionPeriod=1d"
      - "--search.latencyOffset=0s"
      - "--promscrape.config=/etc/victoriametrics/scrape.yaml"
      - "--maxInsertRequestSize=128MB"
      - "--opentelemetry.maxRequestSize=128MB"
      - "--opentelemetry.convertMetricNamesToPrometheus"
      - "--memory.allowedPercent=80"
      - "--search.maxUniqueTimeseries=2500000"
      - "--search.maxConcurrentRequests=32"
      - "--search.maxQueryDuration=20s"
      - "--search.logSlowQueryDuration=10s"

  otel-collector:
    deploy:
      resources:
        limits:
          memory: 1G
          cpus: "1"

  verifier:
    environment:
      - VM_ENDPOINT=http://victoriametrics:8428
      - MG_ENDPOINT=http://metrics-governor:9090
      - CHECK_INTERVAL=15s
      - VERIFICATION_METRIC=generator_verification_counter
      - PASS_THRESHOLD=85.0          # Lower threshold - data will be dropped
      - METRICS_PORT=9092

volumes:
  mg-queue-limits-data:
    driver: local
