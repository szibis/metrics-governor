services:
  # OpenTelemetry Collector - receives metrics from generator
  otel-collector:
    image: otel/opentelemetry-collector-contrib:0.144.0
    ports:
      - "4317:4317"     # OTLP gRPC (for generator)
      - "4318:4318"     # OTLP HTTP (for generator)
      - "8888:8888"     # Prometheus metrics
    volumes:
      - ./test/otel-collector-config.yaml:/etc/otelcol/config.yaml:ro
    command: ["--config=/etc/otelcol/config.yaml"]
    depends_on:
      - metrics-governor
    restart: on-failure

  # metrics-governor - proxy between otel-collector and VictoriaMetrics
  metrics-governor:
    build: .
    ports:
      - "14317:4317"    # gRPC receiver (from otel-collector)
      - "14318:4318"    # HTTP receiver (from otel-collector)
      - "9090:9090"     # Prometheus metrics
    deploy:
      resources:
        limits:
          # Memory limit - auto-detected by Go runtime via cgroups
          # GOMEMLIMIT is automatically set to 90% of this value
          memory: 512M
    command:
      - "-exporter-endpoint=http://victoriametrics:8428/opentelemetry/v1/metrics"
      - "-exporter-protocol=http"
      - "-exporter-insecure=true"
      - "-exporter-compression=zstd"
      - "-stats-labels=service,env"
      - "-limits-config=/etc/metrics-governor/limits.yaml"
      - "-limits-dry-run=true"
      - "-flush-interval=100ms"
      - "-batch-size=100"
      - "-max-batch-bytes=8388608"    # 8MB byte-aware batch splitting
      - "-buffer-size=5000"
      # Performance tuning (inspired by VictoriaMetrics techniques)
      - "-string-interning=true"
      - "-intern-max-value-length=64"
      # Queue configuration - FastQueue persistent retry queue
      - "-queue-enabled=true"
      - "-queue-path=/data/queue"
      - "-queue-max-size=10000"
      - "-queue-max-bytes=536870912"
      - "-queue-retry-interval=5s"
      - "-queue-full-behavior=drop_oldest"
      - "-queue-adaptive-enabled=true"
      - "-queue-inmemory-blocks=256"
      - "-queue-chunk-size=134217728"
      - "-queue-meta-sync=1s"
      - "-queue-stale-flush=5s"
      # Queue resilience - backoff and circuit breaker
      - "-queue-backoff-enabled=true"
      - "-queue-backoff-multiplier=2.0"
      - "-queue-circuit-breaker-enabled=true"
      - "-queue-circuit-breaker-threshold=10"
      - "-queue-circuit-breaker-reset-timeout=30s"
      # Memory limit - auto-detect container limits
      - "-memory-limit-ratio=0.9"
      # Rule cache - LRU cache for metric-to-rule matching
      - "-rule-cache-max-size=10000"
      # Sharding configuration - consistent hashing across endpoints
      - "-sharding-enabled=true"
      - "-sharding-headless-service=victoriametrics:8428"
      - "-sharding-labels=service,env"
      - "-sharding-virtual-nodes=150"
      - "-sharding-fallback-on-empty=true"
      - "-sharding-dns-refresh-interval=30s"
    volumes:
      - ./examples/limits.yaml:/etc/metrics-governor/limits.yaml:ro
      - mg-queue-data:/data/queue
    depends_on:
      - victoriametrics
    restart: on-failure

  # VictoriaMetrics for metrics storage (OTLP HTTP endpoint)
  # Default: moderate resources. Use docker-compose.perf.yaml for high volume
  victoriametrics:
    image: victoriametrics/victoria-metrics:v1.134.0
    ports:
      - "8428:8428"    # HTTP API and UI
    deploy:
      resources:
        limits:
          memory: 2G   # Moderate memory for default testing
    command:
      - "--storageDataPath=/victoria-metrics-data"
      - "--httpListenAddr=:8428"
      - "--retentionPeriod=1d"
      - "--search.latencyOffset=0s"
      - "--promscrape.config=/etc/victoriametrics/scrape.yaml"
      - "--maxInsertRequestSize=128MB"
      - "--opentelemetry.maxRequestSize=128MB"
      - "--opentelemetry.convertMetricNamesToPrometheus"
      - "--memory.allowedPercent=80"
      - "--search.maxUniqueTimeseries=1000000"
      - "--search.maxConcurrentRequests=32"
      - "--search.maxQueryDuration=10s"
      - "--search.logSlowQueryDuration=5s"
    volumes:
      - ./test/vmscrape-config.yaml:/etc/victoriametrics/scrape.yaml:ro

  # Test metrics generator - sends to otel-collector
  # Default: moderate load. Use docker-compose.light.yaml or docker-compose.perf.yaml for overrides
  metrics-generator:
    build:
      context: ./test
      dockerfile: Dockerfile.generator
    ports:
      - "9091:9091"   # Prometheus metrics
    environment:
      - OTLP_ENDPOINT=otel-collector:4317
      - METRICS_INTERVAL=500ms        # Moderate interval
      - SERVICES=payment-api,order-api,inventory-api,user-api
      - ENVIRONMENTS=prod,staging,dev
      - ENABLE_EDGE_CASES=false
      - ENABLE_HIGH_CARDINALITY=true
      - ENABLE_BURST_TRAFFIC=false
      - ENABLE_DIVERSE_METRICS=true
      - HIGH_CARDINALITY_COUNT=50     # Moderate cardinality
      - DIVERSE_METRIC_COUNT=50       # Moderate diversity
      - BURST_SIZE=500
      - BURST_INTERVAL_SEC=60
      - STATS_INTERVAL_SEC=15
      - TARGET_METRICS_PER_SEC=5000   # Moderate throughput
      - TARGET_DATAPOINTS_PER_SEC=10000
      - METRICS_PORT=9091
    depends_on:
      - otel-collector

  # Data verification tool
  verifier:
    build:
      context: ./test/verifier
      dockerfile: Dockerfile
    ports:
      - "9092:9092"   # Prometheus metrics
    environment:
      - VM_ENDPOINT=http://victoriametrics:8428
      - MG_ENDPOINT=http://metrics-governor:9090
      - CHECK_INTERVAL=15s
      - VERIFICATION_METRIC=generator_verification_counter
      - PASS_THRESHOLD=95.0
      - METRICS_PORT=9092
    depends_on:
      - victoriametrics
      - metrics-governor
      - metrics-generator

  # Grafana for visualization
  grafana:
    image: grafana/grafana:12.3.2
    ports:
      - "3000:3000"
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_AUTH_ANONYMOUS_ENABLED=true
      - GF_AUTH_ANONYMOUS_ORG_ROLE=Viewer
    volumes:
      - ./test/grafana/provisioning:/etc/grafana/provisioning:ro
      - ./test/grafana/dashboards:/var/lib/grafana/dashboards/test:ro
      - ./dashboards:/var/lib/grafana/dashboards/operations:ro
    depends_on:
      - victoriametrics

# Named volumes for persistent data
volumes:
  mg-queue-data:
    driver: local
